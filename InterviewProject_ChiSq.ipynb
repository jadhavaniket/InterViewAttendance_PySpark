{"cells":[{"cell_type":"code","source":["from pyspark.sql import SparkSession\nspark = SparkSession.builder.appName('Logistic1').getOrCreate()"],"metadata":{},"outputs":[],"execution_count":1},{"cell_type":"code","source":["df = spark.read.csv('FileStore/tables/Interview.csv', inferSchema = True, header = True)"],"metadata":{},"outputs":[],"execution_count":2},{"cell_type":"code","source":["df.columns"],"metadata":{},"outputs":[],"execution_count":3},{"cell_type":"code","source":["from pyspark.sql.functions import lower, col\n\ndf = df.withColumn(\"Client\",lower(col('Client name')))\ndf = df.withColumn(\"Industry\",lower(col('Industry')))\ndf = df.withColumn(\"Location\",lower(col('Location')))\ndf = df.withColumn(\"Interview_Type\",lower(col('Interview Type')))\ndf = df.withColumn(\"Cand_Loc\",lower(col('Candidate Current Location')))\ndf = df.withColumn(\"Job_Loc\",lower(col('Candidate Job Location')))\ndf = df.withColumn(\"Venue\",lower(col('Interview Venue')))\ndf = df.withColumn(\"Native\",lower(col('Candidate Native location')))\ndf = df.withColumn(\"Position\",lower(col('Position to be closed')))\ndf = df.withColumn(\"Skillset\",lower(col('Nature of Skillset')))\ndf = df.withColumn(\"Marital_Stat\",lower(col('Marital Status')))\ndf = df.withColumn(\"Expected\",lower(col('Expected Attendance')))\ndf = df.withColumn(\"ObservedAtt\",lower(col('Observed Attendance')))\ndf = df.withColumn(\"candidateID\",lower(col('Name(Cand ID)')))\ndf = df.withColumn(\"Letter_Shared\",lower(col('Has the call letter been shared')))\ndf = df.withColumn(\"Followup_Call\",lower(col('Can I Call you three hours before the interview and follow up on your attendance for the interview')))\ndf = df.withColumn(\"Perm_to_start\",lower(col('Have you obtained the necessary permission to start at the required time')))\ndf = df.withColumn(\"Availability\",lower(col('Hope there will be no unscheduled meetings')))\n\ndf = df.drop('_c23', '_c24', '_c25', '_c26', '_c27')"],"metadata":{},"outputs":[],"execution_count":4},{"cell_type":"code","source":["from pyspark.sql.functions import regexp_replace\nprint(df.toPandas()['Client'].unique())"],"metadata":{},"outputs":[],"execution_count":5},{"cell_type":"code","source":["df = df.withColumn('Client', regexp_replace('Client', 'aon hewitt gurgaon', 'aon hewitt'))\ndf = df.withColumn('Client', regexp_replace('Client', 'aon hewitt', 'hewitt'))\ndf = df.withColumn('Client', regexp_replace('Client', 'hewitt', 'aon hewitt'))\ndf = df.withColumn('Client', regexp_replace('Client', 'standard chartered bank chennai', 'standard chartered bank'))\ndf = df.filter((df['Client']!='\\ufeff\\ufeff'))\nprint(df.toPandas()['Client'].unique())"],"metadata":{},"outputs":[],"execution_count":6},{"cell_type":"code","source":["print(df.toPandas()['Industry'].unique())"],"metadata":{},"outputs":[],"execution_count":7},{"cell_type":"code","source":["df = df.withColumn('Industry', regexp_replace('Industry', 'it products and services', 'it services'))\ndf = df.withColumn('Industry', regexp_replace('Industry', 'it services', 'it'))\nprint(df.toPandas()['Industry'].unique())"],"metadata":{},"outputs":[],"execution_count":8},{"cell_type":"code","source":["print(df.toPandas()['Location'].unique())"],"metadata":{},"outputs":[],"execution_count":9},{"cell_type":"code","source":["df = df.withColumn('Location', regexp_replace('Location', '- cochin- ', 'cochin'))\ndf = df.withColumn('Location', regexp_replace('Location', 'gurgaonr', 'gurgaon'))\ndf = df.withColumn('Location', regexp_replace('Location', 'chennai ', 'chennai'))\nprint(df.toPandas()['Location'].unique())"],"metadata":{},"outputs":[],"execution_count":10},{"cell_type":"code","source":["print(df.toPandas()['Interview_Type'].unique())"],"metadata":{},"outputs":[],"execution_count":11},{"cell_type":"code","source":["df = df.withColumn('Interview_Type', regexp_replace('Interview_Type', 'sceduled walkin', 'scheduled walk in'))\ndf = df.withColumn('Interview_Type', regexp_replace('Interview_Type', 'scheduled walk in', 'scheduled walkin'))\ndf = df.withColumn('Interview_Type', regexp_replace('Interview_Type', 'walkin ', 'walkin'))\ndf = df.withColumn('Interview_Type', regexp_replace('Interview_Type', 'scheduled ', 'scheduled'))\nprint(df.toPandas()['Interview_Type'].unique())"],"metadata":{},"outputs":[],"execution_count":12},{"cell_type":"code","source":["print(df.toPandas()['Cand_Loc'].unique())"],"metadata":{},"outputs":[],"execution_count":13},{"cell_type":"code","source":["df = df.withColumn('Cand_Loc', regexp_replace('Cand_Loc', '- cochin- ', 'cochin'))\ndf = df.withColumn('Cand_Loc', regexp_replace('Cand_Loc', 'chennai ', 'chennai'))\nprint(df.toPandas()['Cand_Loc'].unique())"],"metadata":{},"outputs":[],"execution_count":14},{"cell_type":"code","source":["print(df.toPandas()['Job_Loc'].unique())"],"metadata":{},"outputs":[],"execution_count":15},{"cell_type":"code","source":["df = df.withColumn('Job_Loc', regexp_replace('Job_Loc', '- cochin- ', 'cochin'))\nprint(df.toPandas()['Job_Loc'].unique())"],"metadata":{},"outputs":[],"execution_count":16},{"cell_type":"code","source":["print(df.toPandas()['Venue'].unique())"],"metadata":{},"outputs":[],"execution_count":17},{"cell_type":"code","source":["df = df.withColumn('Venue', regexp_replace('Venue', '- cochin- ', 'cochin'))\nprint(df.toPandas()['Venue'].unique())"],"metadata":{},"outputs":[],"execution_count":18},{"cell_type":"code","source":["print(df.toPandas()['Native'].unique())"],"metadata":{},"outputs":[],"execution_count":19},{"cell_type":"code","source":["df = df.withColumn('Native', regexp_replace('Native', '- cochin- ', 'cochin'))\nprint(df.toPandas()['Native'].unique())"],"metadata":{},"outputs":[],"execution_count":20},{"cell_type":"code","source":["from pyspark.sql import functions as F\n\ndf = df.withColumn( 'New_skillset', F.when(df.Skillset.like(\"%java%\"),\"java\").when(df.Skillset.like(\"%oracle%\"),\"database\").when(df.Skillset.like(\"%sql%\"),\"database\").when(df.Skillset.like(\"%operations%\"),\"operations\").when(df.Skillset.like(\"%fresher%\"),\"fresher\").when(df.Skillset.like(\"%cdd%\"),\"customer_support\").otherwise(\"other\").alias(\"newSkillset\"))\n\ndf.select('New_skillset').distinct().show()"],"metadata":{},"outputs":[],"execution_count":21},{"cell_type":"code","source":["df.select('Position').distinct().show()"],"metadata":{},"outputs":[],"execution_count":22},{"cell_type":"code","source":["df = df.withColumn('Position', regexp_replace('Position', 'production- sterile', 'production-sterile'))\ndf.select('Position').distinct().show()"],"metadata":{},"outputs":[],"execution_count":23},{"cell_type":"code","source":["from pyspark.sql.types import IntegerType\ndf = df.withColumn('candidateID', regexp_replace('candidateID', 'candidate ',''))\ndf = df.withColumn('candidateID', df['candidateID'].cast(IntegerType()))"],"metadata":{},"outputs":[],"execution_count":24},{"cell_type":"code","source":["#replace NA with None and drop na\nfrom pyspark.sql.functions import col, when\n\ndef blank_as_null(x):\n    return when(col(x) != '', col(x)).otherwise(None)"],"metadata":{},"outputs":[],"execution_count":25},{"cell_type":"code","source":["df = df.withColumn('expected', regexp_replace('expected', 'na','')).withColumn('expected', regexp_replace('expected', '11:00 am','yes')).withColumn('expected', regexp_replace('expected', '10.30 am','yes'))\ndf = df.withColumn('expected', blank_as_null('expected'))\ndf.select('Expected').distinct().show()"],"metadata":{},"outputs":[],"execution_count":26},{"cell_type":"code","source":["from pyspark.sql.functions import trim\ndf = df.withColumn('ObservedAtt', trim(df['ObservedAtt']))"],"metadata":{},"outputs":[],"execution_count":27},{"cell_type":"code","source":["df.select('ObservedAtt').distinct().show()"],"metadata":{},"outputs":[],"execution_count":28},{"cell_type":"code","source":["df.columns"],"metadata":{},"outputs":[],"execution_count":29},{"cell_type":"code","source":["df.select('Letter_Shared').distinct().show()"],"metadata":{},"outputs":[],"execution_count":30},{"cell_type":"code","source":["df = df.withColumn('Letter_Shared', regexp_replace('Letter_Shared', 'need to check','uncertain'))\ndf = df.withColumn('Letter_Shared', regexp_replace('Letter_Shared', 'not yet','uncertain'))\ndf = df.withColumn('Letter_Shared', regexp_replace('Letter_Shared', 'havent checked','uncertain'))\ndf = df.withColumn('Letter_Shared', regexp_replace('Letter_Shared', 'not sure','uncertain'))\ndf = df.withColumn('Letter_Shared', regexp_replace('Letter_Shared', 'yet to check','uncertain'))\ndf = df.withColumn('Letter_Shared', regexp_replace('Letter_Shared', 'na',''))\ndf = df.withColumn('Letter_Shared', blank_as_null('Letter_Shared'))\ndf.select('Letter_Shared').distinct().show()"],"metadata":{},"outputs":[],"execution_count":31},{"cell_type":"code","source":["df.select('Followup_Call').distinct().show()"],"metadata":{},"outputs":[],"execution_count":32},{"cell_type":"code","source":["df = df.withColumn('Followup_Call', regexp_replace('Followup_Call', 'no dont','no'))\ndf = df.withColumn('Followup_Call', regexp_replace('Followup_Call', 'na',''))\ndf = df.withColumn('Followup_Call', blank_as_null('Followup_Call'))\ndf.select('Followup_Call').distinct().show()"],"metadata":{},"outputs":[],"execution_count":33},{"cell_type":"code","source":["df.select('Perm_to_start').distinct().show()"],"metadata":{},"outputs":[],"execution_count":34},{"cell_type":"code","source":["df = df.withColumn('Perm_to_start', regexp_replace('Perm_to_start', 'yet to confirm','uncertain')).withColumn('Perm_to_start', regexp_replace('Perm_to_start', 'not yet','uncertain')).withColumn('Perm_to_start', regexp_replace('Perm_to_start', 'na', ''))\ndf = df.withColumn('Perm_to_start', blank_as_null('Perm_to_start'))\n\ndf.select('Perm_to_start').distinct().show()"],"metadata":{},"outputs":[],"execution_count":35},{"cell_type":"code","source":["df.select('Availability').distinct().show()"],"metadata":{},"outputs":[],"execution_count":36},{"cell_type":"code","source":["df = df.withColumn('Availability', regexp_replace('Availability', 'not sure','uncertain')).withColumn('Availability', regexp_replace('Availability', 'cant say','uncertain')).withColumn('Availability', regexp_replace('Availability', 'na', ''))\ndf = df.withColumn('Availability', blank_as_null('Availability'))\n\ndf.select('Availability').distinct().show()"],"metadata":{},"outputs":[],"execution_count":37},{"cell_type":"code","source":["df_cleaned = df.select('Client','Industry','Location','Position','New_skillset','Interview_Type','Gender','Cand_Loc','Job_Loc','Venue','Native','Marital_Stat','ObservedAtt','Letter_Shared','Followup_Call','Perm_to_start','Availability')\n\ndf_cleaned = df_cleaned.na.drop()\ndf_cleaned.show()"],"metadata":{},"outputs":[],"execution_count":38},{"cell_type":"code","source":["from pyspark.ml.feature import VectorAssembler, VectorIndexer, OneHotEncoder, StringIndexer \n\n\nclient_indexer = StringIndexer(inputCol = 'Client', outputCol = 'clientIndex')\nclient_encoder = OneHotEncoder(inputCol = 'clientIndex', outputCol = 'clientVector')\n\nindustry_indexer = StringIndexer(inputCol = 'Industry', outputCol = 'industryIndex')\nindustry_encoder = OneHotEncoder(inputCol = 'industryIndex', outputCol = 'industryVector')\n\nlocation_indexer = StringIndexer(inputCol = 'Location', outputCol = 'locationIndex')\nlocation_encoder = OneHotEncoder(inputCol = 'locationIndex', outputCol = 'locationVector')\n\nposition_indexer = StringIndexer(inputCol = 'Position', outputCol = 'positionIndex')\nposition_encoder = OneHotEncoder(inputCol = 'positionIndex', outputCol = 'positionVector')\n\nskillset_indexer = StringIndexer(inputCol = 'New_skillset', outputCol = 'skillsetIndex')\nskillset_encoder = OneHotEncoder(inputCol = 'skillsetIndex', outputCol = 'skillsetVector')\n\nInterview_Type_indexer = StringIndexer(inputCol = 'Interview_Type', outputCol = 'Interview_TypeIndex')\nInterview_Type_encoder = OneHotEncoder(inputCol = 'Interview_TypeIndex', outputCol = 'Interview_TypeVector')\n\nGender_indexer = StringIndexer(inputCol = 'Gender', outputCol = 'GenderIndex')\nGender_encoder = OneHotEncoder(inputCol = 'GenderIndex', outputCol = 'GenderVector')\n\nCand_Loc_indexer = StringIndexer(inputCol = 'Cand_Loc', outputCol = 'Cand_LocIndex')\nCand_Loc_encoder = OneHotEncoder(inputCol = 'Cand_LocIndex', outputCol = 'Cand_LocVector')\n\nJob_Loc_indexer = StringIndexer(inputCol = 'Job_Loc', outputCol = 'Job_LocIndex')\nJob_Loc_encoder = OneHotEncoder(inputCol = 'Job_LocIndex', outputCol = 'Job_LocVector')\n\nVenue_indexer = StringIndexer(inputCol = 'Venue', outputCol = 'VenueIndex')\nVenue_encoder = OneHotEncoder(inputCol = 'VenueIndex', outputCol = 'VenueVector')\n\nNative_indexer = StringIndexer(inputCol = 'Native', outputCol = 'NativeIndex')\nNative_encoder = OneHotEncoder(inputCol = 'NativeIndex', outputCol = 'NativeVector')\n\nMarital_Stat_indexer = StringIndexer(inputCol = 'Marital_Stat', outputCol = 'Marital_StatIndex')\nMarital_Stat_encoder = OneHotEncoder(inputCol = 'Marital_StatIndex', outputCol = 'Marital_StatVector')\n\nObservedAtt_indexer = StringIndexer(inputCol = 'ObservedAtt', outputCol = 'ObservedAttIndex')\nObservedAtt_encoder = OneHotEncoder(inputCol = 'ObservedAttIndex', outputCol = 'ObservedAttVector')\n\nLetter_Shared_indexer = StringIndexer(inputCol = 'Letter_Shared', outputCol = 'Letter_SharedIndex')\nLetter_Shared_encoder = OneHotEncoder(inputCol = 'Letter_SharedIndex', outputCol = 'Letter_SharedVector')\n\nFollowup_Call_indexer = StringIndexer(inputCol = 'Followup_Call', outputCol = 'Followup_CallIndex')\nFollowup_Call_encoder = OneHotEncoder(inputCol = 'Followup_CallIndex', outputCol = 'Followup_CallVector')\n\nPerm_to_start_indexer = StringIndexer(inputCol = 'Perm_to_start', outputCol = 'Perm_to_startIndex')\nPerm_to_start_encoder = OneHotEncoder(inputCol = 'Perm_to_startIndex', outputCol = 'Perm_to_startVector')\n\nAvailability_indexer = StringIndexer(inputCol = 'Availability', outputCol = 'AvailabilityIndex')\nAvailability_encoder = OneHotEncoder(inputCol = 'AvailabilityIndex', outputCol = 'AvailabilityVector')\n\n#assembler = VectorAssembler(inputCols = ['clientVector', 'industryVector', 'locationVector', 'positionVector', 'skillsetVector', 'Interview_TypeVector', 'GenderVector', 'Cand_LocVector', 'Job_LocVector', 'VenueVector', 'NativeVector', 'Marital_StatVector', 'Letter_SharedVector', 'Followup_CallVector', 'Followup_CallVector', 'Perm_to_startVector', 'AvailabilityVector'], outputCol = 'features')\n\nassembler = VectorAssembler(inputCols = ['clientVector', 'industryVector', 'locationVector', 'skillsetVector', 'Interview_TypeVector', 'GenderVector', 'Cand_LocVector', 'Job_LocVector', 'VenueVector', 'NativeVector', 'Marital_StatVector', 'Letter_SharedVector', 'Followup_CallVector', 'Followup_CallVector', 'Perm_to_startVector', 'AvailabilityVector'], outputCol = 'features')\n"],"metadata":{},"outputs":[],"execution_count":39},{"cell_type":"code","source":["from pyspark.ml import Pipeline\n\npipeline = Pipeline(stages = [client_indexer, industry_indexer, location_indexer, position_indexer, skillset_indexer, Interview_Type_indexer, Gender_indexer, Cand_Loc_indexer, Job_Loc_indexer, Venue_indexer, Native_indexer, Marital_Stat_indexer, ObservedAtt_indexer, Letter_Shared_indexer, Followup_Call_indexer, Perm_to_start_indexer, Availability_indexer, client_encoder, industry_encoder, location_encoder, position_encoder, skillset_encoder, Interview_Type_encoder, Gender_encoder, Cand_Loc_encoder, Job_Loc_encoder, Venue_encoder, Native_encoder, Marital_Stat_encoder, Letter_Shared_encoder, Followup_Call_encoder, Perm_to_start_encoder, Availability_encoder, assembler])"],"metadata":{},"outputs":[],"execution_count":40},{"cell_type":"code","source":["output = pipeline.fit(df_cleaned).transform(df_cleaned)"],"metadata":{},"outputs":[],"execution_count":41},{"cell_type":"code","source":["final_data =output.select('features','ObservedAttIndex')"],"metadata":{},"outputs":[],"execution_count":42},{"cell_type":"code","source":["train_data, test_data = final_data.randomSplit([0.7,0.3])"],"metadata":{},"outputs":[],"execution_count":43},{"cell_type":"code","source":["from pyspark.ml.feature import ChiSqSelector\n\nselector = ChiSqSelector(featuresCol = 'features',\n                         outputCol = 'selectedFeatures', labelCol = \"ObservedAttIndex\", selectorType = 'fpr') #false positive rate < 0.05 by default\n\nselector_model = selector.fit(train_data)\n\nresult = selector_model.transform(train_data)\n\ntrain_data = result.select('ObservedAttIndex', 'selectedFeatures')\ntrain_data = train_data.withColumnRenamed('selectedFeatures', 'features')\n\ntest_data = selector_model.transform(test_data)\ntest_data = test_data.select('ObservedAttIndex','selectedFeatures')\ntest_data = test_data.withColumnRenamed('selectedFeatures','features')"],"metadata":{},"outputs":[],"execution_count":44},{"cell_type":"code","source":["#Logistic Regression\nfrom pyspark.ml.classification import LogisticRegression\nlogistic = LogisticRegression(featuresCol = 'features', labelCol = 'ObservedAttIndex')\nfit_model = logistic.fit(train_data)\nresults = fit_model.transform(test_data)"],"metadata":{},"outputs":[],"execution_count":45},{"cell_type":"code","source":["from pyspark.ml.evaluation import BinaryClassificationEvaluator, MulticlassClassificationEvaluator\nmy_eval = MulticlassClassificationEvaluator(metricName='accuracy', labelCol='ObservedAttIndex')\nmy_eval.evaluate(results)"],"metadata":{},"outputs":[],"execution_count":46},{"cell_type":"code","source":["from pyspark.ml.classification import DecisionTreeClassifier, RandomForestClassifier, GBTClassifier\ndtc = DecisionTreeClassifier(labelCol = \"ObservedAttIndex\", featuresCol = \"features\")\nrfc = RandomForestClassifier(numTrees = 100, labelCol = \"ObservedAttIndex\", featuresCol = \"features\")\ngbc = GBTClassifier(labelCol = \"ObservedAttIndex\", featuresCol = \"features\")\n  \ndtc_model = dtc.fit(train_data)\nrfc_model = rfc.fit(train_data)\ngbc_model = gbc.fit(train_data)\n  \ndtc_results = dtc_model.transform(test_data)\nrfc_results = rfc_model.transform(test_data)\ngbc_results = gbc_model.transform(test_data)\n  \nfrom pyspark.ml.evaluation import MulticlassClassificationEvaluator\n  \nmy_eval = MulticlassClassificationEvaluator(metricName= 'accuracy', labelCol='ObservedAttIndex')"],"metadata":{},"outputs":[],"execution_count":47},{"cell_type":"code","source":["my_eval.evaluate(dtc_results)"],"metadata":{},"outputs":[],"execution_count":48},{"cell_type":"code","source":["my_eval.evaluate(rfc_results)"],"metadata":{},"outputs":[],"execution_count":49},{"cell_type":"code","source":["my_eval.evaluate(gbc_results)"],"metadata":{},"outputs":[],"execution_count":50},{"cell_type":"code","source":["from pyspark.sql import *\n\ndisplay(df_cleaned)"],"metadata":{},"outputs":[],"execution_count":51},{"cell_type":"code","source":["df_skillset = df.select('Skillset', 'New_skillset')\ndf_skillset.groupby(df.Skillset).count().sort(col(\"count\").desc()).show()"],"metadata":{},"outputs":[],"execution_count":52},{"cell_type":"code","source":["display(df_cleaned)"],"metadata":{},"outputs":[],"execution_count":53},{"cell_type":"code","source":["display(df_cleaned)"],"metadata":{},"outputs":[],"execution_count":54},{"cell_type":"code","source":["display(df_cleaned)"],"metadata":{},"outputs":[],"execution_count":55},{"cell_type":"code","source":["display(df_cleaned)"],"metadata":{},"outputs":[],"execution_count":56},{"cell_type":"code","source":["from pyspark.ml.classification import NaiveBayes\nfrom pyspark.ml.evaluation import MulticlassClassificationEvaluator\n\nnb = NaiveBayes(smoothing=1.0, modelType=\"multinomial\", featuresCol = 'features', labelCol = 'ObservedAttIndex')\nmodel = nb.fit(train_data)\n\nnb_results = model.transform(test_data)\n\n# compute accuracy on the test set\n\nmy_eval = MulticlassClassificationEvaluator(metricName= 'accuracy', labelCol='ObservedAttIndex')\nmy_eval.evaluate(nb_results)"],"metadata":{},"outputs":[],"execution_count":57}],"metadata":{"name":"InterviewProject_ChiSq","notebookId":4278028580700462},"nbformat":4,"nbformat_minor":0}
